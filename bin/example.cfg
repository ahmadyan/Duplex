# This is an example configuration file to be used with Duplex.
# this config contains all the possible settings, many of which may overlap.
# therefore should be used as a reference. This config may not work

# A lot of default settings are stored in the file "generic.cfg"
# We include that file by default using @include directive
@include "generic.cfg" @ifExists ;

# simple configuration file

Duplex {
	# The name of the current experiment.
	# This name will also be present on the plots
	name = "Example configuration file";

	# mode defines the operating mode for the Duplex algorithm
	# load: loads the config file from a saved xml file.
	#			The xml file should be defined at "savefile"
	# duplex: executes the random tree optimizer for nonlinear optimization
	# fopt: executes the random tree optimizer for functional optimization
	# opt: executes a generic optimizer class
	# simulated-annealing: executes the simulated annealing algorithm
	# Anything else you'll get the error "Unknown optimization mode is selected"
	mode = "opt";



	iterations = "100";
	output = working_directory + "simple.png";
    savefile = working_directory + "simple.xml" ;
	initial_state_assignment = "random";  #initial states can be either "random" or "assigned"
    initial_temperature = "1";
    initial_step_length = "1";
    plot{
		enable="true";
        uid-plot{
         	type = "tree.parameter";
         	x="0";
        	y="1";
        }
        uid-plot{
         	type = "tree.objective";
        	x="1";
        	y="2";
        }
        uid-plot{
        	type = "distance";
        }
		uid-plot{
			type = "trace";
			x="0";
			y="1";
		}
	};

	simulation{
        engine = "internal";			#["hspice", "internal"];
    };
    
    # optimization scope contains all the settings for the optimization algorithms, such as gradient descent, adams, etc.
    optimization{
        #algorithm chooses which algorithm to be executed.
        #Possible values are {gd, adagrad}
        # Gradient descent is enabled with gd w/ momentum=0, and nestrov=false
        # Gradient Descent with Momentum is enabled with gd + non-zero value for momentum
        # Gradient Descent with Nestrov momentum is endabled with gd + non-zero momentum + nestrov=true
        # Adagrad is enabled with adagrad + autocorrelation=0
        # RMSProp is enabled with adagrad + non-zero autocorrelation
        algorithm="gd";
        
        # Chooses the learning rate (parameter gamma, or alpha) of the gradient descent algorithm.
        learning_rate="0.05";
        
        # The decay parameter for gradient descent.
        # Any value >0 will enable decaying the learnig rate based on number of iteration.
        # we use the 1/t decay equation: learning_rate = learning_rate*(1/decay*iteration) to reduce the learning rate
        # as we progress.
        decay="0.1";
        
        # the optimization algorithm converges when the changes between two successive states are less than epsilon,
        # or when the maximum number of iterations are met.
        epsilon="1e-9";
        
        # the friction value for momemtum update in gradient descent algorithm with momemtun
        # Put 0 to disable momentum, the recommended value is 0.9, recommend grid search on {0.5, 0.9, 0.95, 0.99}
        # momentum can improve the performance of optimization, see "On the importance of initialization and momentum in deep learning" by Sutskever et. al.
        momentum="0.9";
        
        # Enables Nestrov accelerated gradient computation in Gradient Descent Algorithm.
        # Has two options, {true, false}.
        # The Nestrov equation was implemented according to "Advances in optimizing Recurrent Networks" by Yoshua Bengio
        # For more information, refer to "Training recurrent neural networks", by Ilya Sutskever
        nestrov="true";
        
        # The fudge factor (epsilon) used in AdaGrad/RMSProp algorithm to avoid division by zero. Set a small ammount (usually between 1e-4 to 1e-8) would suffice.
        fudge_factor="1e-4";
        
        # Decay-rate or Rho parameter in RMSProp Algorithm. Non-zero value for autocorr turns to vanilla adagrad, typical values include {0.9, 0.95, 0.99, 0.999}
        # The auto correlation weight for the AdaGrad algorithm, autocorr is the moving sum average of previous gradients and the corrent gradient.
        # put 0 to disable autocorr and using the vanilla adagrad.
        # See "Neural Networks for Machine Learning", Lecture 6a, "Overview of mini-batch gradient descent" by Geofry Hinton
        autocorrelation="0.99";
    };

    ############################
	# Parameter list
	############################
    parameter {
        size = "2";
        uid-parameter {
            name = "x";
            comment = "x";
            range {
                min = "-1";
                max = "1";
            };
            init = "0.1";
            unit = "u";
        };

        uid-parameter {
            name = "y";
            comment = "y";
            range {
                min = "-1";
                max = "1";
            };
            init = "-0.2";
            unit = "u";
        };
    };

	############################
	# Performance metric list
	############################
    objective {
        size = "1";
		gradient="true";							#implies gradient information exists

        uid-objective {
            name = "f";
			keyword = "Saddle";

			function = "x*x*x-3*x*y*y";				# x^3-3xy^2
            comment = "Monkey Saddle Point";
            goal {
				mode = "minimize";
                optimum = "-1";
                min = "-1";
                max = "-0.5";
            };
            min = "0";
            max = "50";


			uid-grad {
				name = "df/dx";
				derivative = "3*(x*x-y*y)";
			};
			uid-grad {
				name = "df/dy";
				derivative = "-6*x*y";
			};
        };
    };
};
