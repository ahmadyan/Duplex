# This is an example configuration file to be used with Duplex.
# this config contains all the possible settings, many of which may overlap.
# therefore should be used as a reference. This config may not work

# A lot of default settings are stored in the file "generic.cfg"
# We include that file by default using @include directive
@include "generic.cfg" @ifExists ;

# simple configuration file

Duplex {
	# The name of the current experiment.
	# This name will also be present on the plots
	name = "Example configuration file";

	# mode defines the operating mode for the Duplex algorithm
	# load: loads the config file from a saved xml file.
	#			The xml file should be defined at "savefile"
	# duplex: executes the random tree optimizer for nonlinear optimization
	# fopt: executes the random tree optimizer for functional optimization
	# opt: executes a generic optimizer class
	# simulated-annealing: executes the simulated annealing algorithm
	# Anything else you'll get the error "Unknown optimization mode is selected"
	mode = "opt";



	iterations = "100";
	output = working_directory + "simple.png";
    savefile = working_directory + "simple.xml" ;
	initial_state_assignment = "random";  #initial states can be either "random" or "assigned"
    initial_temperature = "1";
    initial_step_length = "1";
    plot{
		enable="true";
        uid-plot{
         	type = "tree.parameter";
         	x="0";
        	y="1";
        }
        uid-plot{
         	type = "tree.objective";
        	x="1";
        	y="2";
        }
        uid-plot{
        	type = "distance";
        }
		uid-plot{
			type = "trace";
			x="0";
			y="1";
		}
	};

	simulation{
        engine = "internal";			#["hspice", "internal"];
    };
    
    # optimization scope contains all the settings for the optimization algorithms, such as gradient descent, adams, etc.
    optimization{
        #algorithm chooses which algorithm to be executed.
        #Possible algorithms are {gd}
        algorithm="gd";
        # Chooses the learning rate (parameter gamma, or alpha) of the gradient descent algorithm.
        learning_rate="0.05";
    };

    ############################
	# Parameter list
	############################
    parameter {
        size = "2";
        uid-parameter {
            name = "x";
            comment = "x";
            range {
                min = "-1";
                max = "1";
            };
            init = "0.1";
            unit = "u";
        };

        uid-parameter {
            name = "y";
            comment = "y";
            range {
                min = "-1";
                max = "1";
            };
            init = "-0.2";
            unit = "u";
        };
    };

	############################
	# Performance metric list
	############################
    objective {
        size = "1";
		gradient="true";							#implies gradient information exists

        uid-objective {
            name = "f";
			keyword = "Saddle";

			function = "x*x*x-3*x*y*y";				# x^3-3xy^2
            comment = "Monkey Saddle Point";
            goal {
				mode = "minimize";
                optimum = "-1";
                min = "-1";
                max = "-0.5";
            };
            min = "0";
            max = "50";


			uid-grad {
				name = "df/dx";
				derivative = "3*(x*x-y*y)";
			};
			uid-grad {
				name = "df/dy";
				derivative = "-6*x*y";
			};
        };
    };
};
